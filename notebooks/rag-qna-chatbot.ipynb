{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bebad58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b3324c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today? ðŸ˜Š'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1\")\n",
    "\n",
    "def remove_think_tags(text):\n",
    "    import re\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "\n",
    "chain = model | RunnableLambda(remove_think_tags)\n",
    "chain.invoke(\"Hello deepseek!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b90dd31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.011166534386575222,\n",
       " 0.002552350750193,\n",
       " -0.01770380511879921,\n",
       " -0.03453180193901062,\n",
       " -0.012192479334771633,\n",
       " -0.003684643656015396,\n",
       " -0.013600025326013565,\n",
       " 0.0020534538198262453,\n",
       " -0.0031904384959489107,\n",
       " -0.01776636205613613,\n",
       " 0.017753850668668747,\n",
       " 0.011573159135878086,\n",
       " -0.0197807177901268,\n",
       " -0.0034375409595668316,\n",
       " 0.0036064465530216694,\n",
       " 0.012949426658451557,\n",
       " 0.020018436014652252,\n",
       " -0.014400763437151909,\n",
       " -0.0011072697816416621,\n",
       " 0.018091661855578423,\n",
       " -0.018704727292060852,\n",
       " -0.012836822308599949,\n",
       " -0.0063589815981686115,\n",
       " -0.003284274833276868,\n",
       " -0.01592716947197914,\n",
       " 0.002186388708651066,\n",
       " 0.021106939762830734,\n",
       " -0.018542077392339706,\n",
       " 0.02822599560022354,\n",
       " -0.030227839946746826,\n",
       " 0.008670485578477383,\n",
       " -0.003941129893064499,\n",
       " -0.004932668060064316,\n",
       " -0.024272354319691658,\n",
       " 0.01403792854398489,\n",
       " 0.0026070885360240936,\n",
       " 0.006950151175260544,\n",
       " -0.021307123824954033,\n",
       " 0.021407216787338257,\n",
       " -0.0081387460231781,\n",
       " 0.013287237845361233,\n",
       " 0.003387494944036007,\n",
       " 0.013049518689513206,\n",
       " -0.014738574624061584,\n",
       " -0.034256551414728165,\n",
       " 0.01724087819457054,\n",
       " 0.007125312462449074,\n",
       " -0.01855458877980709,\n",
       " -0.008282627910375595,\n",
       " 0.0316791757941246,\n",
       " 0.024985510855913162,\n",
       " -0.004238278605043888,\n",
       " -0.019230211153626442,\n",
       " 0.004741867072880268,\n",
       " -0.01720334403216839,\n",
       " 0.008908204734325409,\n",
       " -0.02997760847210884,\n",
       " 0.016878044232726097,\n",
       " 0.03385617956519127,\n",
       " -0.02664954401552677,\n",
       " 0.01288686878979206,\n",
       " 0.007119056768715382,\n",
       " -0.008069932460784912,\n",
       " 0.0061806924641132355,\n",
       " -0.0036815155763179064,\n",
       " 0.0009985759388655424,\n",
       " 0.01069109607487917,\n",
       " 0.011748320423066616,\n",
       " -0.02083168551325798,\n",
       " 0.030252862721681595,\n",
       " 0.006362109445035458,\n",
       " -0.008076188154518604,\n",
       " 0.006474713329225779,\n",
       " -0.010672328993678093,\n",
       " 0.013950347900390625,\n",
       " 0.00897076167166233,\n",
       " -0.017403528094291687,\n",
       " 0.011078953742980957,\n",
       " 0.004128802567720413,\n",
       " 0.0008562573348172009,\n",
       " 0.029852494597434998,\n",
       " -0.03851046785712242,\n",
       " -0.00809495523571968,\n",
       " 0.017115764319896698,\n",
       " 0.021569866687059402,\n",
       " -0.0038785722572356462,\n",
       " -0.005282990634441376,\n",
       " 0.007444356102496386,\n",
       " -0.018967468291521072,\n",
       " -0.03675885498523712,\n",
       " 0.007300473749637604,\n",
       " 0.007250427734106779,\n",
       " 0.015476753935217857,\n",
       " 0.005874160211533308,\n",
       " -0.004917028360068798,\n",
       " 0.01568944938480854,\n",
       " -0.000797218584921211,\n",
       " 0.02120703086256981,\n",
       " -0.000911777256987989,\n",
       " -0.03473198786377907,\n",
       " -0.007069010753184557,\n",
       " -0.010797444730997086,\n",
       " -0.004322731401771307,\n",
       " -0.015902146697044373,\n",
       " -0.012092387303709984,\n",
       " -0.003547016764059663,\n",
       " -0.008245093747973442,\n",
       " -0.00237718946300447,\n",
       " 0.027150005102157593,\n",
       " -0.0010916304308921099,\n",
       " -0.01691557839512825,\n",
       " 0.013312260620296001,\n",
       " -0.010434609837830067,\n",
       " -0.03438166528940201,\n",
       " -0.0028823420871049166,\n",
       " 0.0019955879542976618,\n",
       " -0.002591449301689863,\n",
       " 0.00027486251201480627,\n",
       " -0.007744632661342621,\n",
       " -0.009846569038927555,\n",
       " 0.018567100167274475,\n",
       " 0.02927696332335472,\n",
       " 0.01811668463051319,\n",
       " -0.026048991829156876,\n",
       " 0.006505991797894239,\n",
       " 0.008951994590461254,\n",
       " -0.03363097459077835,\n",
       " -0.014663505367934704,\n",
       " -0.0031357004772871733,\n",
       " -0.007256683427840471,\n",
       " 0.020694058388471603,\n",
       " 0.001156533951871097,\n",
       " 0.0023146318271756172,\n",
       " 0.004988969769328833,\n",
       " -0.032529961317777634,\n",
       " 0.016177399083971977,\n",
       " -0.007600750308483839,\n",
       " 0.009658895432949066,\n",
       " -0.01059100404381752,\n",
       " -0.007775911595672369,\n",
       " 0.006606084294617176,\n",
       " 0.022633345797657967,\n",
       " 0.005117212887853384,\n",
       " -0.01837942749261856,\n",
       " -0.006102495361119509,\n",
       " 0.00811997801065445,\n",
       " 0.009558803401887417,\n",
       " -0.005430000834167004,\n",
       " 0.0014153660740703344,\n",
       " -0.0024241076316684484,\n",
       " 0.007819701917469501,\n",
       " 0.012861846014857292,\n",
       " 0.006743710953742266,\n",
       " -0.0003088782250415534,\n",
       " 0.019205188378691673,\n",
       " 0.026399314403533936,\n",
       " -0.005064039025455713,\n",
       " 0.0316041074693203,\n",
       " 0.0014396071201190352,\n",
       " -0.00021777869551442564,\n",
       " 0.0011416764464229345,\n",
       " -0.00292144063860178,\n",
       " 0.000918033008929342,\n",
       " -0.0373343862593174,\n",
       " 0.006255761720240116,\n",
       " 0.01890491135418415,\n",
       " 0.011448043398559093,\n",
       " -0.003004329511895776,\n",
       " -0.019793229177594185,\n",
       " -0.020206110551953316,\n",
       " -0.020093506202101707,\n",
       " 0.009127155877649784,\n",
       " -0.03838535398244858,\n",
       " 0.0252732764929533,\n",
       " -0.011473067104816437,\n",
       " 0.008945738896727562,\n",
       " 0.008482812903821468,\n",
       " 0.01881732977926731,\n",
       " -0.017491109669208527,\n",
       " -0.017828920856118202,\n",
       " -0.020356247201561928,\n",
       " 0.007256683427840471,\n",
       " 0.012761753983795643,\n",
       " 0.03132885321974754,\n",
       " -0.011560647748410702,\n",
       " -0.010672328993678093,\n",
       " 0.01498880423605442,\n",
       " -0.018454495817422867,\n",
       " 0.004216383211314678,\n",
       " -0.006831291597336531,\n",
       " 0.018592122942209244,\n",
       " 0.03260502964258194,\n",
       " 0.03545765578746796,\n",
       " 0.012273804284632206,\n",
       " -0.6786249876022339,\n",
       " -0.0058272420428693295,\n",
       " 0.03345581144094467,\n",
       " 0.020794151350855827,\n",
       " 0.014238113537430763,\n",
       " 0.009039575234055519,\n",
       " 0.007594494614750147,\n",
       " 0.02030620165169239,\n",
       " 0.005442512687295675,\n",
       " 0.020493874326348305,\n",
       " -0.0166778601706028,\n",
       " 0.017541155219078064,\n",
       " -0.017278414219617844,\n",
       " -0.006302679888904095,\n",
       " -0.011942248791456223,\n",
       " -0.008520347066223621,\n",
       " 0.006818780209869146,\n",
       " -0.02024364471435547,\n",
       " -0.013524956069886684,\n",
       " 0.013387329876422882,\n",
       " -0.014525878243148327,\n",
       " 0.029176872223615646,\n",
       " -0.014050440862774849,\n",
       " -0.006287040188908577,\n",
       " 0.00605557719245553,\n",
       " 0.011147767305374146,\n",
       " 0.01632753759622574,\n",
       " -0.009371130727231503,\n",
       " -0.010378308594226837,\n",
       " 0.02787567302584648,\n",
       " -0.006227610632777214,\n",
       " 0.017278414219617844,\n",
       " -0.0058929272927343845,\n",
       " 0.01341235265135765,\n",
       " 0.06771235913038254,\n",
       " 0.0069126165471971035,\n",
       " -0.0058897994458675385,\n",
       " -0.0006685844855383039,\n",
       " 0.01421308983117342,\n",
       " 0.03753456845879555,\n",
       " -0.017803898081183434,\n",
       " -0.016164887696504593,\n",
       " 0.008539114147424698,\n",
       " -0.015939680859446526,\n",
       " 0.002366241766139865,\n",
       " 0.017103252932429314,\n",
       " 0.007807190530002117,\n",
       " 0.003359343856573105,\n",
       " -0.008651718497276306,\n",
       " -0.0005989891942590475,\n",
       " 0.01796654798090458,\n",
       " -0.001591309322975576,\n",
       " 0.006987685803323984,\n",
       " 0.01049091201275587,\n",
       " -0.013324772007763386,\n",
       " -0.0010509679559618235,\n",
       " 0.014713550917804241,\n",
       " 0.0056020342744886875,\n",
       " -0.0015709780855104327,\n",
       " 0.01386276725679636,\n",
       " 0.000641215534415096,\n",
       " 0.028276043012738228,\n",
       " -0.0043978001922369,\n",
       " 0.004472869448363781,\n",
       " -0.018679704517126083,\n",
       " 0.023972077295184135,\n",
       " -0.017228366807103157,\n",
       " 0.010741142556071281,\n",
       " -0.014951270073652267,\n",
       " -0.018742261454463005,\n",
       " -0.0011690454557538033,\n",
       " -0.00872053112834692,\n",
       " -0.014625970274209976,\n",
       " -0.008576649241149426,\n",
       " 0.018779795616865158,\n",
       " 0.024347424507141113,\n",
       " 0.009471222758293152,\n",
       " -0.005289246328175068,\n",
       " -0.002292736666277051,\n",
       " 0.011604437604546547,\n",
       " 0.01560186967253685,\n",
       " 0.019568022340536118,\n",
       " -0.020869219675660133,\n",
       " -0.019993413239717484,\n",
       " 0.02483537420630455,\n",
       " -0.011910970322787762,\n",
       " -0.026199128478765488,\n",
       " -0.008332674391567707,\n",
       " 0.008351441472768784,\n",
       " 0.0033624719362705946,\n",
       " 0.03540761023759842,\n",
       " 0.003756584832444787,\n",
       " -0.0016953113954514265,\n",
       " -0.02182009629905224,\n",
       " 0.03750954568386078,\n",
       " 0.0045166597701609135,\n",
       " -0.011979782953858376,\n",
       " -0.0015615944284945726,\n",
       " 0.03330567479133606,\n",
       " 0.0006505991914309561,\n",
       " 0.01225503720343113,\n",
       " 0.008044909685850143,\n",
       " -0.002300556283444166,\n",
       " 0.01211115438491106,\n",
       " 0.014713550917804241,\n",
       " 0.01228631567209959,\n",
       " 0.002389700850471854,\n",
       " 0.029226917773485184,\n",
       " 0.034231528639793396,\n",
       " -0.017491109669208527,\n",
       " -0.0011682634940370917,\n",
       " 0.009727708995342255,\n",
       " -0.037409454584121704,\n",
       " 0.01901751570403576,\n",
       " 0.010540958493947983,\n",
       " -0.03067825548350811,\n",
       " 0.0063589815981686115,\n",
       " 0.016665348783135414,\n",
       " 0.03105360083281994,\n",
       " -0.018767284229397774,\n",
       " 0.02267087996006012,\n",
       " -0.0010861565824598074,\n",
       " 0.011529368348419666,\n",
       " -0.0064184111542999744,\n",
       " -0.007938561029732227,\n",
       " 0.007444356102496386,\n",
       " -0.014000394381582737,\n",
       " 0.008126234635710716,\n",
       " -0.01945541799068451,\n",
       " 0.0001620633265702054,\n",
       " 0.012011062353849411,\n",
       " -0.011166534386575222,\n",
       " -0.003099729772657156,\n",
       " 0.0020956802181899548,\n",
       " -0.006662386003881693,\n",
       " -0.004879494197666645,\n",
       " -0.0021285228431224823,\n",
       " -0.008783088997006416,\n",
       " 0.005345548037439585,\n",
       " 0.02138219214975834,\n",
       " -0.004447846673429012,\n",
       " 0.00566771999001503,\n",
       " -0.007131568156182766,\n",
       " -0.0010267269099131227,\n",
       " 0.016690371558070183,\n",
       " -0.018479520455002785,\n",
       " -0.0202186219394207,\n",
       " -0.0015725420089438558,\n",
       " 0.014851178042590618,\n",
       " -0.010703608393669128,\n",
       " 0.007657052017748356,\n",
       " -0.016752928495407104,\n",
       " -0.03575793281197548,\n",
       " 0.03052811697125435,\n",
       " -0.007982351817190647,\n",
       " 0.0037816078402101994,\n",
       " 0.011216580867767334,\n",
       " -0.024447515606880188,\n",
       " -0.03543263301253319,\n",
       " -0.02599894441664219,\n",
       " -0.003753456985577941,\n",
       " 0.024372447282075882,\n",
       " -0.01136671844869852,\n",
       " 0.004513531923294067,\n",
       " -0.005545732565224171,\n",
       " -0.028150927275419235,\n",
       " -0.03237982094287872,\n",
       " 0.014600947499275208,\n",
       " 0.003196694189682603,\n",
       " -0.03328065201640129,\n",
       " 0.011967271566390991,\n",
       " -0.006812524516135454,\n",
       " -0.025410903617739677,\n",
       " 0.007688330952078104,\n",
       " -0.013987882994115353,\n",
       " 0.014338205568492413,\n",
       " -0.01650269888341427,\n",
       " -0.016139864921569824,\n",
       " -0.009220992214977741,\n",
       " -0.011579414829611778,\n",
       " 0.0017125146696344018,\n",
       " -0.011454299092292786,\n",
       " -0.0019267745083197951,\n",
       " 0.01111648790538311,\n",
       " 0.01829184591770172,\n",
       " 0.005085933953523636,\n",
       " -0.006377748679369688,\n",
       " 0.03385617956519127,\n",
       " -0.023671802133321762,\n",
       " 0.007738376967608929,\n",
       " 0.01034077350050211,\n",
       " 0.004760634619742632,\n",
       " -0.02208283729851246,\n",
       " -0.002643059240654111,\n",
       " 0.0010189071763306856,\n",
       " -0.007744632661342621,\n",
       " -0.003013713052496314,\n",
       " 0.003634597407653928,\n",
       " 0.01755366660654545,\n",
       " 0.0081387460231781,\n",
       " 0.028125904500484467,\n",
       " 0.009208480827510357,\n",
       " 0.01577703095972538,\n",
       " -0.02223297581076622,\n",
       " -0.013011983595788479,\n",
       " -0.03453180193901062,\n",
       " -0.0016718523111194372,\n",
       " -0.02842617966234684,\n",
       " 0.011760831810534,\n",
       " 0.003934874199330807,\n",
       " 0.018717238679528236,\n",
       " -0.007006452884525061,\n",
       " 0.008926971815526485,\n",
       " 0.019005004316568375,\n",
       " 0.00750065827742219,\n",
       " 0.031178714707493782,\n",
       " 0.012480244040489197,\n",
       " -0.016114842146635056,\n",
       " -0.030202817171812057,\n",
       " -0.011066442355513573,\n",
       " -0.002677466021850705,\n",
       " 0.009590082801878452,\n",
       " -0.013562491163611412,\n",
       " -0.004175720736384392,\n",
       " 0.018504543229937553,\n",
       " 0.03393125161528587,\n",
       " -0.005692742764949799,\n",
       " 0.025873830541968346,\n",
       " 0.0006032899837009609,\n",
       " -0.034081388264894485,\n",
       " -0.01700315997004509,\n",
       " 0.0037315618246793747,\n",
       " 0.014250624924898148,\n",
       " 0.018429473042488098,\n",
       " -0.008851902559399605,\n",
       " -0.0019611811731010675,\n",
       " 0.008657974191009998,\n",
       " -0.023421570658683777,\n",
       " 0.02614908292889595,\n",
       " 0.009652639739215374,\n",
       " -0.0036033187061548233,\n",
       " 0.003762840526178479,\n",
       " 0.018692215904593468,\n",
       " -0.00936487503349781,\n",
       " 0.006643618922680616,\n",
       " 0.018967468291521072,\n",
       " 0.01796654798090458,\n",
       " -0.010916303843259811,\n",
       " -0.007663307711482048,\n",
       " 0.025573553517460823,\n",
       " -0.00661859568208456,\n",
       " 0.007331752683967352,\n",
       " -0.007588238921016455,\n",
       " -0.0020769129041582346,\n",
       " 0.00791979394853115,\n",
       " -0.0268247053027153,\n",
       " 0.0017656886484473944,\n",
       " -0.02577373757958412,\n",
       " 0.02071908302605152,\n",
       " 0.03142894431948662,\n",
       " 0.020181085914373398,\n",
       " 0.006950151175260544,\n",
       " -0.007444356102496386,\n",
       " -0.013024495914578438,\n",
       " 0.0014317873865365982,\n",
       " -0.009577570483088493,\n",
       " -0.0022536381147801876,\n",
       " -0.013037007302045822,\n",
       " 0.002933952258899808,\n",
       " -0.0024976127315312624,\n",
       " -0.008889436721801758,\n",
       " -0.015376661904156208,\n",
       " 0.015051362104713917,\n",
       " -0.022107861936092377,\n",
       " 0.010002962313592434,\n",
       " 0.0002383054088568315,\n",
       " -0.0069751739501953125,\n",
       " -0.008057421073317528,\n",
       " -0.004441590514034033,\n",
       " 0.0007295781979337335,\n",
       " -0.0038754441775381565,\n",
       " -0.016315026208758354,\n",
       " 0.005655208602547646,\n",
       " 0.017778873443603516,\n",
       " 0.008170024491846561,\n",
       " -0.019480440765619278,\n",
       " -0.02442249283194542,\n",
       " 0.0038785722572356462,\n",
       " 0.0014450809685513377,\n",
       " 0.0018673448357731104,\n",
       " -0.015013827942311764,\n",
       " 0.017015671357512474,\n",
       " 0.016052283346652985,\n",
       " -0.030428024008870125,\n",
       " 0.005301757715642452,\n",
       " 0.0029495914932340384,\n",
       " 0.01656525582075119,\n",
       " 0.005589522887021303,\n",
       " 0.002538275206461549,\n",
       " -0.0037315618246793747,\n",
       " 0.023909520357847214,\n",
       " 0.019442906603217125,\n",
       " 0.006437178701162338,\n",
       " -0.015364150516688824,\n",
       " 0.018354404717683792,\n",
       " 0.0010658253449946642,\n",
       " -0.007369287312030792,\n",
       " -0.0028651386965066195,\n",
       " -0.004303963854908943,\n",
       " 0.004181976430118084,\n",
       " -0.0006638927152380347,\n",
       " 0.0012628819094970822,\n",
       " -0.01316212210804224,\n",
       " -0.00330929784104228,\n",
       " 0.009296061471104622,\n",
       " 0.0005419053486548364,\n",
       " -0.008532858453691006,\n",
       " -0.004350882023572922,\n",
       " 0.02238311432301998,\n",
       " 0.0006478623254224658,\n",
       " -0.013725141063332558,\n",
       " -0.006274528801441193,\n",
       " -0.03310548886656761,\n",
       " -0.011360462754964828,\n",
       " 0.07101540267467499,\n",
       " -0.007256683427840471,\n",
       " -0.0071503352373838425,\n",
       " 0.01837942749261856,\n",
       " -0.021219542250037193,\n",
       " 0.001739101717248559,\n",
       " -0.031103646382689476,\n",
       " -0.01685302145779133,\n",
       " -0.016264980658888817,\n",
       " -0.007075266446918249,\n",
       " -0.0031529038678854704,\n",
       " -0.0036252138670533895,\n",
       " -0.005232944618910551,\n",
       " 0.018779795616865158,\n",
       " 0.014425786212086678,\n",
       " 0.0024976127315312624,\n",
       " 0.0015326616121456027,\n",
       " -0.02080666273832321,\n",
       " -0.01654023304581642,\n",
       " 0.012230013497173786,\n",
       " -0.007369287312030792,\n",
       " 0.007125312462449074,\n",
       " 0.0020049714948982,\n",
       " 0.016765441745519638,\n",
       " 0.02629922144114971,\n",
       " -0.0003292094625066966,\n",
       " 0.010090542957186699,\n",
       " 0.01571447215974331,\n",
       " -0.01211115438491106,\n",
       " -0.002835423918440938,\n",
       " 0.006455945782363415,\n",
       " -0.005764684174209833,\n",
       " 0.012330105528235435,\n",
       " 0.0037190502043813467,\n",
       " 0.015476753935217857,\n",
       " 0.006674897391349077,\n",
       " 0.00301996897906065,\n",
       " 0.024234820157289505,\n",
       " 0.009690174832940102,\n",
       " 0.005039015784859657,\n",
       " 0.011097720824182034,\n",
       " 0.0010853746207430959,\n",
       " -0.00327176321297884,\n",
       " -0.013499933294951916,\n",
       " 0.012161200866103172,\n",
       " 0.00640589976683259,\n",
       " -0.011848412454128265,\n",
       " 0.017478598281741142,\n",
       " 0.0022207952570170164,\n",
       " -0.025523507967591286,\n",
       " 0.028501249849796295,\n",
       " 0.005642696749418974,\n",
       " 0.003912978805601597,\n",
       " -0.03140392154455185,\n",
       " -0.02704991213977337,\n",
       " 0.021632423624396324,\n",
       " -0.0166778601706028,\n",
       " -0.009959172457456589,\n",
       " -0.012467732653021812,\n",
       " -0.016840510070323944,\n",
       " -0.014375739730894566,\n",
       " -0.016815487295389175,\n",
       " -0.011654484085738659,\n",
       " -0.004375905264168978,\n",
       " -0.022708414122462273,\n",
       " -0.02579876035451889,\n",
       " -0.028451204299926758,\n",
       " -0.004216383211314678,\n",
       " -0.020794151350855827,\n",
       " -0.0033155535347759724,\n",
       " -0.0028854699339717627,\n",
       " -0.033756088465452194,\n",
       " 0.0003368336474522948,\n",
       " 0.006233866326510906,\n",
       " 0.019392861053347588,\n",
       " 0.005633313208818436,\n",
       " 0.0029589752666652203,\n",
       " 0.0017125146696344018,\n",
       " 0.004888877738267183,\n",
       " 0.018979979678988457,\n",
       " -0.006987685803323984,\n",
       " -0.020618990063667297,\n",
       " 0.01108520943671465,\n",
       " -0.00585852051153779,\n",
       " 0.01639009453356266,\n",
       " 0.0015858355909585953,\n",
       " 0.0002899154496844858,\n",
       " 0.011416764929890633,\n",
       " -0.01846700720489025,\n",
       " 0.028301065787672997,\n",
       " 0.01341235265135765,\n",
       " 0.017340971156954765,\n",
       " 0.026048991829156876,\n",
       " -0.003406262258067727,\n",
       " 0.0013926889514550567,\n",
       " 0.001997151877731085,\n",
       " 0.0043164752423763275,\n",
       " 0.005592650733888149,\n",
       " -0.014788620173931122,\n",
       " 0.007156591396778822,\n",
       " -0.007369287312030792,\n",
       " -0.012436454184353352,\n",
       " -0.001180774997919798,\n",
       " -0.0027228202670812607,\n",
       " 0.005770939867943525,\n",
       " 0.014300670474767685,\n",
       " 0.009652639739215374,\n",
       " 0.021720003336668015,\n",
       " -0.017453575506806374,\n",
       " 0.001556120696477592,\n",
       " 0.033030420541763306,\n",
       " -0.029677333310246468,\n",
       " 0.01190471462905407,\n",
       " 0.002117575379088521,\n",
       " -0.0011768651893362403,\n",
       " 0.017778873443603516,\n",
       " 0.022533252835273743,\n",
       " 0.032179638743400574,\n",
       " -0.010328262113034725,\n",
       " -0.02369682490825653,\n",
       " -0.01333728339523077,\n",
       " -0.021657446399331093,\n",
       " 0.017478598281741142,\n",
       " 0.024472538381814957,\n",
       " 0.013499933294951916,\n",
       " 0.011141511611640453,\n",
       " -0.002087860368192196,\n",
       " -0.017778873443603516,\n",
       " 0.007832213304936886,\n",
       " -0.013099564239382744,\n",
       " -0.00968391913920641,\n",
       " 0.01610233075916767,\n",
       " -0.005852264817804098,\n",
       " -0.0060680885799229145,\n",
       " -0.02468523569405079,\n",
       " 0.0067124320194125175,\n",
       " -0.011491834186017513,\n",
       " 0.000865640991833061,\n",
       " -0.00796358473598957,\n",
       " -0.011729552410542965,\n",
       " -0.021807584911584854,\n",
       " -0.017841432243585587,\n",
       " -0.0013817413710057735,\n",
       " -0.010046753101050854,\n",
       " -0.036708809435367584,\n",
       " -0.04461609199643135,\n",
       " -0.016490187495946884,\n",
       " 0.010828723199665546,\n",
       " -0.023534175008535385,\n",
       " 0.025223230943083763,\n",
       " -0.017115764319896698,\n",
       " 0.0005106265307404101,\n",
       " -0.01761622354388237,\n",
       " -0.011748320423066616,\n",
       " -0.004591729026287794,\n",
       " -0.01837942749261856,\n",
       " -0.011035162955522537,\n",
       " -0.011604437604546547,\n",
       " 0.032830238342285156,\n",
       " 0.025723692029714584,\n",
       " 0.028626365587115288,\n",
       " 0.013712629675865173,\n",
       " 0.013825233094394207,\n",
       " 0.006315191276371479,\n",
       " -0.005370571278035641,\n",
       " -0.011579414829611778,\n",
       " -0.0037847356870770454,\n",
       " -0.016052283346652985,\n",
       " -0.011473067104816437,\n",
       " 0.013750163838267326,\n",
       " 0.022620834410190582,\n",
       " 0.01680297590792179,\n",
       " 0.0020409421995282173,\n",
       " 0.008507835678756237,\n",
       " 0.004644902888685465,\n",
       " -0.006568549666553736,\n",
       " -0.02269590273499489,\n",
       " 0.0049232845194637775,\n",
       " -0.013875278644263744,\n",
       " 0.001420839806087315,\n",
       " -0.02190767601132393,\n",
       " 0.005060911178588867,\n",
       " 0.001826682360842824,\n",
       " 0.01682799868285656,\n",
       " 0.005323653109371662,\n",
       " -0.01645265333354473,\n",
       " 0.01732845976948738,\n",
       " 0.008895693346858025,\n",
       " 0.02474779263138771,\n",
       " 0.00457921763882041,\n",
       " 0.03105360083281994,\n",
       " -0.01221124641597271,\n",
       " 0.02299617975950241,\n",
       " 0.014588436111807823,\n",
       " 0.010228170081973076,\n",
       " -0.006165052764117718,\n",
       " 0.009402409195899963,\n",
       " -0.011973527260124683,\n",
       " 0.009271038696169853,\n",
       " 0.009477478452026844,\n",
       " 0.013912813737988472,\n",
       " 0.02807585708796978,\n",
       " -0.02080666273832321,\n",
       " -0.009271038696169853,\n",
       " -0.002361549995839596,\n",
       " 0.021369680762290955,\n",
       " -0.001735973870381713,\n",
       " 0.005301757715642452,\n",
       " 0.004072500858455896,\n",
       " -0.01864216849207878,\n",
       " 0.005135980434715748,\n",
       " -0.02059396728873253,\n",
       " 0.0013191837351769209,\n",
       " -0.030052678659558296,\n",
       " 0.005996147636324167,\n",
       " -0.008951994590461254,\n",
       " -0.02564862184226513,\n",
       " 0.018504543229937553,\n",
       " -0.003916106652468443,\n",
       " -0.02647438272833824,\n",
       " -0.005298629868775606,\n",
       " -0.005173514597117901,\n",
       " 0.04111286625266075,\n",
       " 0.003981792367994785,\n",
       " 0.021407216787338257,\n",
       " 0.012630382552742958,\n",
       " -0.007519425358623266,\n",
       " -0.003199822036549449,\n",
       " 0.016089819371700287,\n",
       " -0.010828723199665546,\n",
       " -0.009402409195899963,\n",
       " 0.038760699331760406,\n",
       " -0.01542670838534832,\n",
       " -0.01263663824647665,\n",
       " -0.018779795616865158,\n",
       " 0.002000279724597931,\n",
       " 0.005292374175041914,\n",
       " -0.01702818274497986,\n",
       " -0.025148160755634308,\n",
       " 0.008044909685850143,\n",
       " 0.019593045115470886,\n",
       " -0.0007768874056637287,\n",
       " -0.021106939762830734,\n",
       " 0.013637560419738293,\n",
       " -0.04746871814131737,\n",
       " 0.03333069756627083,\n",
       " -0.00022403446200769395,\n",
       " -0.031629130244255066,\n",
       " -0.013137099333107471,\n",
       " 0.017491109669208527,\n",
       " -0.022533252835273743,\n",
       " 0.017691293731331825,\n",
       " -0.03718424588441849,\n",
       " 0.027450282126665115,\n",
       " 0.024897931143641472,\n",
       " -0.01116027869284153,\n",
       " -0.018892399966716766,\n",
       " 0.007738376967608929,\n",
       " -0.001427095616236329,\n",
       " -0.007119056768715382,\n",
       " -0.003750329138711095,\n",
       " 0.02740023471415043,\n",
       " -0.007331752683967352,\n",
       " 0.01206736359745264,\n",
       " 0.027124982327222824,\n",
       " 0.0016921835485845804,\n",
       " -0.031103646382689476,\n",
       " -0.008357697166502476,\n",
       " -0.01630251482129097,\n",
       " 0.01767878234386444,\n",
       " -0.015789542347192764,\n",
       " -0.003268635366111994,\n",
       " -0.014588436111807823,\n",
       " 0.014188067056238651,\n",
       " 0.000634959782473743,\n",
       " 0.01787896640598774,\n",
       " -0.01720334403216839,\n",
       " -0.01403792854398489,\n",
       " 0.0019627450965344906,\n",
       " -0.004122546873986721,\n",
       " 0.029176872223615646,\n",
       " -0.0018767284927889705,\n",
       " -0.0020956802181899548,\n",
       " -0.00710654491558671,\n",
       " -0.01343737542629242,\n",
       " -0.023033713921904564,\n",
       " -0.039861712604761124,\n",
       " 0.0006134556024335325,\n",
       " 0.0002486665325704962,\n",
       " -0.02787567302584648,\n",
       " -0.005942973308265209,\n",
       " -0.00644343439489603,\n",
       " 0.005379954818636179,\n",
       " 0.023484129458665848,\n",
       " 0.010328262113034725,\n",
       " -0.010246937163174152,\n",
       " 0.020618990063667297,\n",
       " 0.020093506202101707,\n",
       " -0.010816211812198162,\n",
       " -0.010002962313592434,\n",
       " 0.011404253542423248,\n",
       " 0.006437178701162338,\n",
       " -0.029952585697174072,\n",
       " 0.014150532893836498,\n",
       " 0.005254839546978474,\n",
       " -0.013424864038825035,\n",
       " 7.976096094353124e-05,\n",
       " -0.017115764319896698,\n",
       " -0.007857236079871655,\n",
       " 0.004588601179420948,\n",
       " 0.005442512687295675,\n",
       " -0.004407184198498726,\n",
       " -0.008326418697834015,\n",
       " -0.004526043310761452,\n",
       " 0.04416567459702492,\n",
       " 0.021832607686519623,\n",
       " 0.013750163838267326,\n",
       " -0.00964012835174799,\n",
       " -0.007838468998670578,\n",
       " 0.03308046609163284,\n",
       " -0.009677663445472717,\n",
       " 0.010347029194235802,\n",
       " -0.0014615022810176015,\n",
       " -0.005173514597117901,\n",
       " 0.016415119171142578,\n",
       " -0.005517581477761269,\n",
       " -0.012511523440480232,\n",
       " 0.001925210584886372,\n",
       " -0.02614908292889595,\n",
       " -0.013912813737988472,\n",
       " -0.005201665684580803,\n",
       " 0.005201665684580803,\n",
       " 0.0026821577921509743,\n",
       " -0.01928025670349598,\n",
       " -0.0399618037045002,\n",
       " -0.018091661855578423,\n",
       " -0.01562689244747162,\n",
       " 0.008332674391567707,\n",
       " -0.003371855476871133,\n",
       " -0.021457262337207794,\n",
       " 0.002087860368192196,\n",
       " -0.006837547291070223,\n",
       " 0.015013827942311764,\n",
       " -0.0008296703454107046,\n",
       " 0.005673975683748722,\n",
       " -0.00619320385158062,\n",
       " -0.015726985409855843,\n",
       " -0.01720334403216839,\n",
       " -0.032179638743400574,\n",
       " -0.0175912007689476,\n",
       " -0.009296061471104622,\n",
       " 0.02562359906733036,\n",
       " 0.009489989839494228,\n",
       " -0.011598181910812855,\n",
       " -0.013887790963053703,\n",
       " -0.006662386003881693,\n",
       " -0.03483207896351814,\n",
       " -0.03848544508218765,\n",
       " -0.0026102163828909397,\n",
       " -0.0031779268756508827,\n",
       " 0.002644623164087534,\n",
       " 0.014901223592460155,\n",
       " -0.002120703225955367,\n",
       " 0.03368102014064789,\n",
       " 0.00010322006710339338,\n",
       " 0.006981429643929005,\n",
       " -0.018629657104611397,\n",
       " -0.020318713039159775,\n",
       " 0.010228170081973076,\n",
       " 0.005799090955406427,\n",
       " 0.02056894451379776,\n",
       " -0.002929260255768895,\n",
       " -0.03465691953897476,\n",
       " 0.0003221717197448015,\n",
       " 0.01055972557514906,\n",
       " 0.01411299780011177,\n",
       " -0.0011432403698563576,\n",
       " 0.012824310921132565,\n",
       " -0.017391016706824303,\n",
       " -0.009909125976264477,\n",
       " -0.01717832125723362,\n",
       " -0.003234228817746043,\n",
       " -0.003922362346202135,\n",
       " 0.009671407751739025,\n",
       " 0.014688528142869473,\n",
       " -0.015214012004435062,\n",
       " -0.00385667709633708,\n",
       " -0.0026680822484195232,\n",
       " -0.003675259882584214,\n",
       " 0.019568022340536118,\n",
       " 0.0014599383575841784,\n",
       " 0.014513366855680943,\n",
       " -0.01168576255440712,\n",
       " 0.01446332037448883,\n",
       " -0.02577373757958412,\n",
       " -0.012642893940210342,\n",
       " 0.007225404493510723,\n",
       " -0.012936915270984173,\n",
       " 0.023671802133321762,\n",
       " -0.023509152233600616,\n",
       " 0.006262017413973808,\n",
       " 0.020080994814634323,\n",
       " 0.014588436111807823,\n",
       " 0.011172790080308914,\n",
       " 0.00404747761785984,\n",
       " -0.006743710953742266,\n",
       " 0.010678584687411785,\n",
       " 0.007807190530002117,\n",
       " -0.009702686220407486,\n",
       " -0.013850255869328976,\n",
       " -0.009952916763722897,\n",
       " -0.010847490280866623,\n",
       " -0.0217325147241354,\n",
       " -0.03300539776682854,\n",
       " -0.0036971550434827805,\n",
       " -0.027600420638918877,\n",
       " -0.006262017413973808,\n",
       " -0.011103976517915726,\n",
       " 0.028626365587115288,\n",
       " 0.006083728279918432,\n",
       " -0.022658368572592735,\n",
       " -0.017816409468650818,\n",
       " -0.01689055562019348,\n",
       " 0.020906755700707436,\n",
       " 0.013987882994115353,\n",
       " -0.008232582360506058,\n",
       " -0.003565784078091383,\n",
       " -0.0076195173896849155,\n",
       " -0.0032311007380485535,\n",
       " 0.01752864383161068,\n",
       " -0.01700315997004509,\n",
       " 0.004013070836663246,\n",
       " 0.01411299780011177,\n",
       " 0.019442906603217125,\n",
       " -0.003381239017471671,\n",
       " 0.028100881725549698,\n",
       " 0.24162253737449646,\n",
       " -0.02258329838514328,\n",
       " 0.0003880527219735086,\n",
       " 0.022745948284864426,\n",
       " 0.02126958966255188,\n",
       " 0.01689055562019348,\n",
       " 0.007863492704927921,\n",
       " 0.0065622939728200436,\n",
       " 0.008670485578477383,\n",
       " 0.012624126859009266,\n",
       " -0.012974449433386326,\n",
       " -0.0020518898963928223,\n",
       " -0.004854470957070589,\n",
       " 0.0038879557978361845,\n",
       " 0.008476557210087776,\n",
       " 0.0076445406302809715,\n",
       " -0.020343735814094543,\n",
       " -0.019330302253365517,\n",
       " -0.028200972825288773,\n",
       " -0.025748714804649353,\n",
       " 0.0022677136585116386,\n",
       " -0.030428024008870125,\n",
       " -0.0221704188734293,\n",
       " -0.01560186967253685,\n",
       " 0.018542077392339706,\n",
       " -0.01106018666177988,\n",
       " -0.012674172408878803,\n",
       " 0.0035094821359962225,\n",
       " 0.02805083431303501,\n",
       " 0.014863689430058002,\n",
       " 0.005551988258957863,\n",
       " -0.009952916763722897,\n",
       " 0.016928089782595634,\n",
       " 0.010953838005661964,\n",
       " -0.01393783651292324,\n",
       " -0.011310417205095291,\n",
       " 0.0316791757941246,\n",
       " 0.01411299780011177,\n",
       " 0.025698669254779816,\n",
       " -0.0022567659616470337,\n",
       " -0.017340971156954765,\n",
       " -0.01875477284193039,\n",
       " -0.0166778601706028,\n",
       " -0.014838666655123234,\n",
       " 0.012899380177259445,\n",
       " 0.016352560371160507,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Embedding model intialization\n",
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "\n",
    "embedding_model = init_embedding_model(model_name = \"text-embedding-ada-002\")\n",
    "\n",
    "embedding_model.embed_query(\"Hello world!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "319bdaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53,\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-26T09:17:33+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-26T09:17:33+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Deepseek_V3.pdf', 'total_pages': 53, 'page': 0, 'page_label': '1'}, page_content='DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../data/Deepseek_V3.pdf\")\n",
    "\n",
    "docs = loader.load()\n",
    "len(docs), docs[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "052327e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    persist_directory= \"../vectorstores/deepseek\",\n",
    "    embedding=embedding_model,\n",
    "    documents=docs\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6471ba33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a80189f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fetching tool from vecctor store\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def retrive(query:str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retriver = vector_store.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\" : 4})\n",
    "    retrieved_docs = retriver.invoke(query)\n",
    "    serealized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "\n",
    "    return retrieved_docs, serealized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30c17608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.graph.message import BaseMessage, add_messages\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated = [Sequence[BaseMessage], add_messages]\n",
    "    chat_history:str\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc608878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful Ai who answers best to your ability\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "]\n",
    "\n",
    "template = ChatPromptTemplate(\n",
    "    messages=messages,\n",
    "    input_variables = [\"langauge\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a60102a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi there! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 8, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BQg7hoaoLmzJ5FqD8QD5kvvlRv0Bz', 'finish_reason': 'stop', 'logprobs': None}, id='run-cdb78463-246d-4c3e-8c19-b781ac690e48-0', usage_metadata={'input_tokens': 8, 'output_tokens': 11, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.langchain.init_models import init_llm\n",
    "\n",
    "llm = init_llm(model_name = \"gpt-4o\")\n",
    "\n",
    "llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98eb4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "\n",
    "def query_or_respond(state:MessagesState):\n",
    "    llm_with_tools = llm.bind_tools([retrive])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\" : response}\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrive])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "016ccfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. think user as a newbie and knows nothing about the topic\"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know.\"\n",
    "        \n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e033a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77ec70eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13309e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is GRPO explain it to layman in depth\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrive (call_cMlojhp0csaDw6Dps7awplGj)\n",
      " Call ID: call_cMlojhp0csaDw6Dps7awplGj\n",
      "  Args:\n",
      "    query: GRPO meaning and explanation\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrive\n",
      "\n",
      "([Document(metadata={'page': 29, 'subject': '', 'author': '', 'title': '', 'trapped': '/False', 'creator': 'LaTeX with hyperref', 'page_label': '30', 'creationdate': '2024-12-26T09:17:33+00:00', 'total_pages': 53, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../data/Deepseek_V3.pdf', 'producer': 'pdfTeX-1.40.26', 'moddate': '2024-12-26T09:17:33+00:00', 'keywords': ''}, page_content='as inputs. The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its\\nreliability, we construct preference data that not only provides the final reward but also includes\\nthe chain-of-thought leading to the reward. This approach helps mitigate the risk of reward\\nhacking in specific tasks.\\n5.2.2. Group Relative Policy Optimization\\nSimilar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimiza-\\ntion (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same\\nsize as the policy model, and estimates the baseline from group scores instead. Specifically, for\\neach question ð‘ž, GRPO samples a group of outputs {ð‘œ1, ð‘œ2, Â·Â·Â· , ð‘œðº}from the old policy model\\nðœ‹ðœƒð‘œð‘™ð‘‘ and then optimizes the policy model ðœ‹ðœƒ by maximizing the following objective:\\nJðºð‘…ð‘ƒð‘‚ (ðœƒ)= E[ð‘žâˆ¼ð‘ƒ(ð‘„), {ð‘œð‘–}ðº\\nð‘–=1 âˆ¼ðœ‹ðœƒð‘œð‘™ð‘‘ (ð‘‚|ð‘ž)]\\n1\\nðº\\nðºâˆ‘ï¸\\nð‘–=1\\n\\x12\\nmin\\n\\x12 ðœ‹ðœƒ(ð‘œð‘–|ð‘ž)\\nðœ‹ðœƒð‘œð‘™ð‘‘ (ð‘œð‘–|ð‘ž)ð´ð‘–, clip\\n\\x12 ðœ‹ðœƒ(ð‘œð‘–|ð‘ž)\\nðœ‹ðœƒð‘œð‘™ð‘‘ (ð‘œð‘–|ð‘ž), 1âˆ’ðœ€, 1+ðœ€\\n\\x13\\nð´ð‘–\\n\\x13\\nâˆ’ð›½Dð¾ð¿\\n\\x00\\nðœ‹ðœƒ||ðœ‹ð‘Ÿð‘’ð‘“\\n\\x01\\x13\\n, (26)\\nDð¾ð¿\\n\\x00\\nðœ‹ðœƒ||ðœ‹ð‘Ÿð‘’ð‘“\\n\\x01 =\\nðœ‹ð‘Ÿð‘’ð‘“ (ð‘œð‘–|ð‘ž)\\nðœ‹ðœƒ(ð‘œð‘–|ð‘ž) âˆ’log\\nðœ‹ð‘Ÿð‘’ð‘“ (ð‘œð‘–|ð‘ž)\\nðœ‹ðœƒ(ð‘œð‘–|ð‘ž) âˆ’1, (27)\\nwhere ðœ€and ð›½are hyper-parameters; ðœ‹ð‘Ÿð‘’ð‘“ is the reference model; and ð´ð‘– is the advantage, derived\\nfrom the rewards {ð‘Ÿ1, ð‘Ÿ2, ... , ð‘Ÿðº}corresponding to the outputs within each group:\\nð´ð‘– = ð‘Ÿð‘– âˆ’mean({ð‘Ÿ1, ð‘Ÿ2, Â·Â·Â· , ð‘Ÿðº})\\nstd({ð‘Ÿ1, ð‘Ÿ2, Â·Â·Â· , ð‘Ÿðº}) . (28)\\nWe incorporate prompts from diverse domains, such as coding, math, writing, role-playing,\\nand question answering, during the RL process. This approach not only aligns the model more\\nclosely with human preferences but also enhances performance on benchmarks, especially in\\nscenarios where available SFT data are limited.\\n5.3. Evaluations\\n5.3.1. Evaluation Settings\\nEvaluation Benchmarks. Apart from the benchmark we used for base model testing, we\\nfurther evaluate instructed models on IFEval (Zhou et al., 2023), FRAMES (Krishna et al.,\\n2024), LongBench v2 (Bai et al., 2024), GPQA (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-\\nSimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1, LiveCodeBench (Jain\\net al., 2024) (questions from August 2024 to November 2024), Codeforces 2, Chinese National\\nHigh School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics\\nExamination 2024 (AIME 2024) (MAA, 2024).\\nCompared Baselines. We conduct comprehensive evaluations of our chat model against sev-\\neral strong baselines, including DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5 72B Instruct,\\nLLaMA-3.1 405B Instruct, Claude-Sonnet-3.5-1022, and GPT-4o-0513. For the DeepSeek-V2\\nmodel series, we select the most representative variants for comparison. For closed-source\\nmodels, evaluations are performed through their respective APIs.\\n1https://aider.chat\\n2https://codeforces.com\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\\n30'), Document(metadata={'subject': '', 'author': '', 'creationdate': '2024-12-26T09:17:33+00:00', 'trapped': '/False', 'source': '../data/Deepseek_V3.pdf', 'title': '', 'creator': 'LaTeX with hyperref', 'moddate': '2024-12-26T09:17:33+00:00', 'producer': 'pdfTeX-1.40.26', 'page_label': '17', 'page': 16, 'keywords': '', 'total_pages': 53, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'}, page_content='be efficiently implemented.\\nNotably, our fine-grained quantization strategy is highly consistent with the idea of mi-\\ncroscaling formats (Rouhani et al., 2023b), while the Tensor Cores of NVIDIA next-generation\\nGPUs (Blackwell series) have announced the support for microscaling formats with smaller\\nquantization granularity (NVIDIA, 2024a). We hope our design can serve as a reference for\\nfuture work to keep pace with the latest GPU architectures.\\nIncreasing Accumulation Precision. Low-precision GEMM operations often suffer from un-\\nderflow issues, and their accuracy largely depends on high-precision accumulation, which is\\ncommonly performed in an FP32 precision (Kalamkar et al., 2019; Narang et al., 2017). However,\\nwe observe that the accumulation precision of FP8 GEMM on NVIDIA H800 GPUs is limited to\\nretaining around 14 bits, which is significantly lower than FP32 accumulation precision. This\\nproblem will become more pronounced when the inner dimension K is large (Wortsman et al.,\\n2023), a typical scenario in large-scale model training where the batch size and model width\\nare increased. Taking GEMM operations of two random matrices with K = 4096 for example, in\\nour preliminary test, the limited accumulation precision in Tensor Cores results in a maximum\\nrelative error of nearly 2%. Despite these problems, the limited accumulation precision is still\\nthe default option in a few FP8 frameworks (NVIDIA, 2024b), severely constraining the training\\naccuracy.\\nIn order to address this issue, we adopt the strategy of promotion to CUDA Cores for\\nhigher precision (Thakkar et al., 2023). The process is illustrated in Figure 7 (b). To be specific,\\nduring MMA (Matrix Multiply-Accumulate) execution on Tensor Cores, intermediate results\\nare accumulated using the limited bit width. Once an interval of ð‘ð¶ is reached, these partial\\nresults will be copied to FP32 registers on CUDA Cores, where full-precision FP32 accumulation\\nis performed. As mentioned before, our fine-grained quantization applies per-group scaling\\nfactors along the inner dimension K. These scaling factors can be efficiently multiplied on the\\nCUDA Cores as the dequantization process with minimal additional computational cost.\\nIt is worth noting that this modification reduces the WGMMA (Warpgroup-level Matrix\\nMultiply-Accumulate) instruction issue rate for a single warpgroup. However, on the H800\\narchitecture, it is typical for two WGMMA to persist concurrently: while one warpgroup\\nperforms the promotion operation, the other is able to execute the MMA operation. This design\\nenables overlapping of the two operations, maintaining high utilization of Tensor Cores. Based\\non our experiments, setting ð‘ð¶ = 128 elements, equivalent to 4 WGMMAs, represents the\\nminimal accumulation interval that can significantly improve precision without introducing\\nsubstantial overhead.\\nMantissa over Exponents. In contrast to the hybrid FP8 format adopted by prior work\\n(NVIDIA, 2024b; Peng et al., 2023b; Sun et al., 2019b), which uses E4M3 (4-bit exponent and\\n3-bit mantissa) in Fprop and E5M2 (5-bit exponent and 2-bit mantissa) in Dgrad and Wgrad,\\nwe adopt the E4M3 format on all tensors for higher precision. We attribute the feasibility of\\nthis approach to our fine-grained quantization strategy, i.e., tile and block-wise scaling. By\\noperating on smaller element groups, our methodology effectively shares exponent bits among\\nthese grouped elements, mitigating the impact of the limited dynamic range.\\nOnline Quantization. Delayed quantization is employed in tensor-wise quantization frame-\\nworks (NVIDIA, 2024b; Peng et al., 2023b), which maintains a history of the maximum absolute\\n17'), Document(metadata={'producer': 'pdfTeX-1.40.26', 'title': '', 'source': '../data/Deepseek_V3.pdf', 'moddate': '2024-12-26T09:17:33+00:00', 'page_label': '16', 'subject': '', 'author': '', 'creationdate': '2024-12-26T09:17:33+00:00', 'creator': 'LaTeX with hyperref', 'page': 15, 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'total_pages': 53, 'keywords': ''}, page_content='Scaling Factor\\nâ€¦\\nâ€¦\\nâ€¦\\nâ€¦\\nTensor Core\\nCUDA Core\\nInput\\nScaling \\nFactor\\nWeight\\nScaling \\nFactor\\nOutput\\nTensor Core\\nWGMMA 1\\n WGMMA 4\\nLow Prec Acc\\nCUDA Core FP32 Register\\nInterval\\nOutput\\n/ GEMM Input\\n(b) Increasing accumulation precision(a) Fine-grained quantization\\nFigure 7 |(a) We propose a fine-grained quantization method to mitigate quantization errors\\ncaused by feature outliers; for illustration simplicity, onlyFprop is illustrated. (b) In conjunction\\nwith our quantization strategy, we improve the FP8 GEMM precision by promoting to CUDA\\nCores at an interval of ð‘ð¶ = 128 elements MMA for the high-precision accumulation.\\nthese high-precision components incur some memory overheads, their impact can be minimized\\nthrough efficient sharding across multiple DP ranks in our distributed training system.\\n3.3.2. Improved Precision from Quantization and Multiplication\\nBased on our mixed precision FP8 framework, we introduce several strategies to enhance low-\\nprecision training accuracy, focusing on both the quantization method and the multiplication\\nprocess.\\nFine-Grained Quantization. In low-precision training frameworks, overflows and underflows\\nare common challenges due to the limited dynamic range of the FP8 format, which is constrained\\nby its reduced exponent bits. As a standard practice, the input distribution is aligned to the\\nrepresentable range of the FP8 format by scaling the maximum absolute value of the input\\ntensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes low-\\nprecision training highly sensitive to activation outliers, which can heavily degrade quantization\\naccuracy. To solve this, we propose a fine-grained quantization method that applies scaling\\nat a more granular level. As illustrated in Figure 7 (a), (1) for activations, we group and\\nscale elements on a 1x128 tile basis (i.e., per token per 128 channels); and (2) for weights, we\\ngroup and scale elements on a 128x128 block basis (i.e., per 128 input channels per 128 output\\nchannels). This approach ensures that the quantization process can better accommodate outliers\\nby adapting the scale according to smaller groups of elements. In Appendix B.2, we further\\ndiscuss the training instability when we group and scale activations on a block basis in the same\\nway as weights quantization.\\nOne key modification in our method is the introduction of per-group scaling factors along\\nthe inner dimension of GEMM operations. This functionality is not directly supported in the\\nstandard FP8 GEMM. However, combined with our precise FP32 accumulation strategy, it can\\n16'), Document(metadata={'moddate': '2024-12-26T09:17:33+00:00', 'keywords': '', 'producer': 'pdfTeX-1.40.26', 'page_label': '15', 'author': '', 'trapped': '/False', 'page': 14, 'creator': 'LaTeX with hyperref', 'subject': '', 'creationdate': '2024-12-26T09:17:33+00:00', 'title': '', 'total_pages': 53, 'source': '../data/Deepseek_V3.pdf', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'}, page_content='Î£\\nFprop\\nFP32\\nInput To FP8\\nBF16\\nWeight\\nÎ£\\nDgrad\\nFP32\\nInput \\nGradient\\nOutput\\nOutput \\nGradient\\nBF16\\nTo FP8\\nÎ£\\nWgrad\\nFP32\\nTo FP8\\nTo FP8\\nWeight \\nGradient\\nOptimizer \\nStates\\n  To\\nBF16\\nMaster \\nWeight\\nTo FP8\\nTo BF16\\nTo BF16\\nTo FP32\\næˆ–è€…Input->Activation_L\\nOutput->Activation_{L+1}\\nFP32\\nFigure 6 |The overall mixed precision framework with FP8 data format. For clarification, only\\nthe Linear operator is illustrated.\\npre-training (Fishman et al., 2024). To address this challenge and effectively extend the dynamic\\nrange of the FP8 format, we introduce a fine-grained quantization strategy: tile-wise grouping\\nwith 1 Ã—ð‘ð‘ elements or block-wise grouping with ð‘ð‘ Ã—ð‘ð‘ elements. The associated dequantiza-\\ntion overhead is largely mitigated under our increased-precision accumulation process, a critical\\naspect for achieving accurate FP8 General Matrix Multiplication (GEMM). Moreover, to further\\nreduce memory and communication overhead in MoE training, we cache and dispatch activa-\\ntions in FP8, while storing low-precision optimizer states in BF16. We validate the proposed FP8\\nmixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeek-\\nV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably,\\ncompared with the BF16 baseline, the relative loss error of our FP8-training model remains\\nconsistently below 0.25%, a level well within the acceptable range of training randomness.\\n3.3.1. Mixed Precision Framework\\nBuilding upon widely adopted techniques in low-precision training (Kalamkar et al., 2019;\\nNarang et al., 2017), we propose a mixed precision framework for FP8 training. In this frame-\\nwork, most compute-density operations are conducted in FP8, while a few key operations\\nare strategically maintained in their original data formats to balance training efficiency and\\nnumerical stability. The overall framework is illustrated in Figure 6.\\nFirstly, in order to accelerate model training, the majority of core computation kernels, i.e.,\\nGEMM operations, are implemented in FP8 precision. These GEMM operations accept FP8\\ntensors as inputs and produce outputs in BF16 or FP32. As depicted in Figure 6, all three GEMMs\\nassociated with theLinear operator, namelyFprop (forward pass), Dgrad (activation backward\\npass), and Wgrad (weight backward pass), are executed in FP8. This design theoretically doubles\\nthe computational speed compared with the original BF16 method. Additionally, the FP8 Wgrad\\nGEMM allows activations to be stored in FP8 for use in the backward pass. This significantly\\nreduces memory consumption.\\nDespite the efficiency advantage of the FP8 format, certain operators still require a higher\\nprecision due to their sensitivity to low-precision computations. Besides, some low-cost opera-\\ntors can also utilize a higher precision with a negligible overhead to the overall training cost. For\\nthis reason, after careful investigations, we maintain the original precision (e.g., BF16 or FP32)\\nfor the following components: the embedding module, the output head, MoE gating modules,\\nnormalization operators, and attention operators. These targeted retentions of high precision\\nensure stable training dynamics for DeepSeek-V3. To further guarantee numerical stability, we\\nstore the master weights, weight gradients, and optimizer states in higher precision. While\\n15')], \"Source: {'page': 29, 'subject': '', 'author': '', 'title': '', 'trapped': '/False', 'creator': 'LaTeX with hyperref', 'page_label': '30', 'creationdate': '2024-12-26T09:17:33+00:00', 'total_pages': 53, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'source': '../data/Deepseek_V3.pdf', 'producer': 'pdfTeX-1.40.26', 'moddate': '2024-12-26T09:17:33+00:00', 'keywords': ''}\\nContent: as inputs. The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its\\nreliability, we construct preference data that not only provides the final reward but also includes\\nthe chain-of-thought leading to the reward. This approach helps mitigate the risk of reward\\nhacking in specific tasks.\\n5.2.2. Group Relative Policy Optimization\\nSimilar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimiza-\\ntion (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same\\nsize as the policy model, and estimates the baseline from group scores instead. Specifically, for\\neach question ð‘ž, GRPO samples a group of outputs {ð‘œ1, ð‘œ2, Â·Â·Â· , ð‘œðº}from the old policy model\\nðœ‹ðœƒð‘œð‘™ð‘‘ and then optimizes the policy model ðœ‹ðœƒ by maximizing the following objective:\\nJðºð‘…ð‘ƒð‘‚ (ðœƒ)= E[ð‘žâˆ¼ð‘ƒ(ð‘„), {ð‘œð‘–}ðº\\nð‘–=1 âˆ¼ðœ‹ðœƒð‘œð‘™ð‘‘ (ð‘‚|ð‘ž)]\\n1\\nðº\\nðºâˆ‘ï¸\\nð‘–=1\\n\\x12\\nmin\\n\\x12 ðœ‹ðœƒ(ð‘œð‘–|ð‘ž)\\nðœ‹ðœƒð‘œð‘™ð‘‘ (ð‘œð‘–|ð‘ž)ð´ð‘–, clip\\n\\x12 ðœ‹ðœƒ(ð‘œð‘–|ð‘ž)\\nðœ‹ðœƒð‘œð‘™ð‘‘ (ð‘œð‘–|ð‘ž), 1âˆ’ðœ€, 1+ðœ€\\n\\x13\\nð´ð‘–\\n\\x13\\nâˆ’ð›½Dð¾ð¿\\n\\x00\\nðœ‹ðœƒ||ðœ‹ð‘Ÿð‘’ð‘“\\n\\x01\\x13\\n, (26)\\nDð¾ð¿\\n\\x00\\nðœ‹ðœƒ||ðœ‹ð‘Ÿð‘’ð‘“\\n\\x01 =\\nðœ‹ð‘Ÿð‘’ð‘“ (ð‘œð‘–|ð‘ž)\\nðœ‹ðœƒ(ð‘œð‘–|ð‘ž) âˆ’log\\nðœ‹ð‘Ÿð‘’ð‘“ (ð‘œð‘–|ð‘ž)\\nðœ‹ðœƒ(ð‘œð‘–|ð‘ž) âˆ’1, (27)\\nwhere ðœ€and ð›½are hyper-parameters; ðœ‹ð‘Ÿð‘’ð‘“ is the reference model; and ð´ð‘– is the advantage, derived\\nfrom the rewards {ð‘Ÿ1, ð‘Ÿ2, ... , ð‘Ÿðº}corresponding to the outputs within each group:\\nð´ð‘– = ð‘Ÿð‘– âˆ’mean({ð‘Ÿ1, ð‘Ÿ2, Â·Â·Â· , ð‘Ÿðº})\\nstd({ð‘Ÿ1, ð‘Ÿ2, Â·Â·Â· , ð‘Ÿðº}) . (28)\\nWe incorporate prompts from diverse domains, such as coding, math, writing, role-playing,\\nand question answering, during the RL process. This approach not only aligns the model more\\nclosely with human preferences but also enhances performance on benchmarks, especially in\\nscenarios where available SFT data are limited.\\n5.3. Evaluations\\n5.3.1. Evaluation Settings\\nEvaluation Benchmarks. Apart from the benchmark we used for base model testing, we\\nfurther evaluate instructed models on IFEval (Zhou et al., 2023), FRAMES (Krishna et al.,\\n2024), LongBench v2 (Bai et al., 2024), GPQA (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-\\nSimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1, LiveCodeBench (Jain\\net al., 2024) (questions from August 2024 to November 2024), Codeforces 2, Chinese National\\nHigh School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics\\nExamination 2024 (AIME 2024) (MAA, 2024).\\nCompared Baselines. We conduct comprehensive evaluations of our chat model against sev-\\neral strong baselines, including DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5 72B Instruct,\\nLLaMA-3.1 405B Instruct, Claude-Sonnet-3.5-1022, and GPT-4o-0513. For the DeepSeek-V2\\nmodel series, we select the most representative variants for comparison. For closed-source\\nmodels, evaluations are performed through their respective APIs.\\n1https://aider.chat\\n2https://codeforces.com\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\\n30\\n\\nSource: {'subject': '', 'author': '', 'creationdate': '2024-12-26T09:17:33+00:00', 'trapped': '/False', 'source': '../data/Deepseek_V3.pdf', 'title': '', 'creator': 'LaTeX with hyperref', 'moddate': '2024-12-26T09:17:33+00:00', 'producer': 'pdfTeX-1.40.26', 'page_label': '17', 'page': 16, 'keywords': '', 'total_pages': 53, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'}\\nContent: be efficiently implemented.\\nNotably, our fine-grained quantization strategy is highly consistent with the idea of mi-\\ncroscaling formats (Rouhani et al., 2023b), while the Tensor Cores of NVIDIA next-generation\\nGPUs (Blackwell series) have announced the support for microscaling formats with smaller\\nquantization granularity (NVIDIA, 2024a). We hope our design can serve as a reference for\\nfuture work to keep pace with the latest GPU architectures.\\nIncreasing Accumulation Precision. Low-precision GEMM operations often suffer from un-\\nderflow issues, and their accuracy largely depends on high-precision accumulation, which is\\ncommonly performed in an FP32 precision (Kalamkar et al., 2019; Narang et al., 2017). However,\\nwe observe that the accumulation precision of FP8 GEMM on NVIDIA H800 GPUs is limited to\\nretaining around 14 bits, which is significantly lower than FP32 accumulation precision. This\\nproblem will become more pronounced when the inner dimension K is large (Wortsman et al.,\\n2023), a typical scenario in large-scale model training where the batch size and model width\\nare increased. Taking GEMM operations of two random matrices with K = 4096 for example, in\\nour preliminary test, the limited accumulation precision in Tensor Cores results in a maximum\\nrelative error of nearly 2%. Despite these problems, the limited accumulation precision is still\\nthe default option in a few FP8 frameworks (NVIDIA, 2024b), severely constraining the training\\naccuracy.\\nIn order to address this issue, we adopt the strategy of promotion to CUDA Cores for\\nhigher precision (Thakkar et al., 2023). The process is illustrated in Figure 7 (b). To be specific,\\nduring MMA (Matrix Multiply-Accumulate) execution on Tensor Cores, intermediate results\\nare accumulated using the limited bit width. Once an interval of ð‘ð¶ is reached, these partial\\nresults will be copied to FP32 registers on CUDA Cores, where full-precision FP32 accumulation\\nis performed. As mentioned before, our fine-grained quantization applies per-group scaling\\nfactors along the inner dimension K. These scaling factors can be efficiently multiplied on the\\nCUDA Cores as the dequantization process with minimal additional computational cost.\\nIt is worth noting that this modification reduces the WGMMA (Warpgroup-level Matrix\\nMultiply-Accumulate) instruction issue rate for a single warpgroup. However, on the H800\\narchitecture, it is typical for two WGMMA to persist concurrently: while one warpgroup\\nperforms the promotion operation, the other is able to execute the MMA operation. This design\\nenables overlapping of the two operations, maintaining high utilization of Tensor Cores. Based\\non our experiments, setting ð‘ð¶ = 128 elements, equivalent to 4 WGMMAs, represents the\\nminimal accumulation interval that can significantly improve precision without introducing\\nsubstantial overhead.\\nMantissa over Exponents. In contrast to the hybrid FP8 format adopted by prior work\\n(NVIDIA, 2024b; Peng et al., 2023b; Sun et al., 2019b), which uses E4M3 (4-bit exponent and\\n3-bit mantissa) in Fprop and E5M2 (5-bit exponent and 2-bit mantissa) in Dgrad and Wgrad,\\nwe adopt the E4M3 format on all tensors for higher precision. We attribute the feasibility of\\nthis approach to our fine-grained quantization strategy, i.e., tile and block-wise scaling. By\\noperating on smaller element groups, our methodology effectively shares exponent bits among\\nthese grouped elements, mitigating the impact of the limited dynamic range.\\nOnline Quantization. Delayed quantization is employed in tensor-wise quantization frame-\\nworks (NVIDIA, 2024b; Peng et al., 2023b), which maintains a history of the maximum absolute\\n17\\n\\nSource: {'producer': 'pdfTeX-1.40.26', 'title': '', 'source': '../data/Deepseek_V3.pdf', 'moddate': '2024-12-26T09:17:33+00:00', 'page_label': '16', 'subject': '', 'author': '', 'creationdate': '2024-12-26T09:17:33+00:00', 'creator': 'LaTeX with hyperref', 'page': 15, 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'total_pages': 53, 'keywords': ''}\\nContent: Scaling Factor\\nâ€¦\\nâ€¦\\nâ€¦\\nâ€¦\\nTensor Core\\nCUDA Core\\nInput\\nScaling \\nFactor\\nWeight\\nScaling \\nFactor\\nOutput\\nTensor Core\\nWGMMA 1\\n WGMMA 4\\nLow Prec Acc\\nCUDA Core FP32 Register\\nInterval\\nOutput\\n/ GEMM Input\\n(b) Increasing accumulation precision(a) Fine-grained quantization\\nFigure 7 |(a) We propose a fine-grained quantization method to mitigate quantization errors\\ncaused by feature outliers; for illustration simplicity, onlyFprop is illustrated. (b) In conjunction\\nwith our quantization strategy, we improve the FP8 GEMM precision by promoting to CUDA\\nCores at an interval of ð‘ð¶ = 128 elements MMA for the high-precision accumulation.\\nthese high-precision components incur some memory overheads, their impact can be minimized\\nthrough efficient sharding across multiple DP ranks in our distributed training system.\\n3.3.2. Improved Precision from Quantization and Multiplication\\nBased on our mixed precision FP8 framework, we introduce several strategies to enhance low-\\nprecision training accuracy, focusing on both the quantization method and the multiplication\\nprocess.\\nFine-Grained Quantization. In low-precision training frameworks, overflows and underflows\\nare common challenges due to the limited dynamic range of the FP8 format, which is constrained\\nby its reduced exponent bits. As a standard practice, the input distribution is aligned to the\\nrepresentable range of the FP8 format by scaling the maximum absolute value of the input\\ntensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes low-\\nprecision training highly sensitive to activation outliers, which can heavily degrade quantization\\naccuracy. To solve this, we propose a fine-grained quantization method that applies scaling\\nat a more granular level. As illustrated in Figure 7 (a), (1) for activations, we group and\\nscale elements on a 1x128 tile basis (i.e., per token per 128 channels); and (2) for weights, we\\ngroup and scale elements on a 128x128 block basis (i.e., per 128 input channels per 128 output\\nchannels). This approach ensures that the quantization process can better accommodate outliers\\nby adapting the scale according to smaller groups of elements. In Appendix B.2, we further\\ndiscuss the training instability when we group and scale activations on a block basis in the same\\nway as weights quantization.\\nOne key modification in our method is the introduction of per-group scaling factors along\\nthe inner dimension of GEMM operations. This functionality is not directly supported in the\\nstandard FP8 GEMM. However, combined with our precise FP32 accumulation strategy, it can\\n16\\n\\nSource: {'moddate': '2024-12-26T09:17:33+00:00', 'keywords': '', 'producer': 'pdfTeX-1.40.26', 'page_label': '15', 'author': '', 'trapped': '/False', 'page': 14, 'creator': 'LaTeX with hyperref', 'subject': '', 'creationdate': '2024-12-26T09:17:33+00:00', 'title': '', 'total_pages': 53, 'source': '../data/Deepseek_V3.pdf', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'}\\nContent: Î£\\nFprop\\nFP32\\nInput To FP8\\nBF16\\nWeight\\nÎ£\\nDgrad\\nFP32\\nInput \\nGradient\\nOutput\\nOutput \\nGradient\\nBF16\\nTo FP8\\nÎ£\\nWgrad\\nFP32\\nTo FP8\\nTo FP8\\nWeight \\nGradient\\nOptimizer \\nStates\\n  To\\nBF16\\nMaster \\nWeight\\nTo FP8\\nTo BF16\\nTo BF16\\nTo FP32\\næˆ–è€…Input->Activation_L\\nOutput->Activation_{L+1}\\nFP32\\nFigure 6 |The overall mixed precision framework with FP8 data format. For clarification, only\\nthe Linear operator is illustrated.\\npre-training (Fishman et al., 2024). To address this challenge and effectively extend the dynamic\\nrange of the FP8 format, we introduce a fine-grained quantization strategy: tile-wise grouping\\nwith 1 Ã—ð‘ð‘ elements or block-wise grouping with ð‘ð‘ Ã—ð‘ð‘ elements. The associated dequantiza-\\ntion overhead is largely mitigated under our increased-precision accumulation process, a critical\\naspect for achieving accurate FP8 General Matrix Multiplication (GEMM). Moreover, to further\\nreduce memory and communication overhead in MoE training, we cache and dispatch activa-\\ntions in FP8, while storing low-precision optimizer states in BF16. We validate the proposed FP8\\nmixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeek-\\nV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably,\\ncompared with the BF16 baseline, the relative loss error of our FP8-training model remains\\nconsistently below 0.25%, a level well within the acceptable range of training randomness.\\n3.3.1. Mixed Precision Framework\\nBuilding upon widely adopted techniques in low-precision training (Kalamkar et al., 2019;\\nNarang et al., 2017), we propose a mixed precision framework for FP8 training. In this frame-\\nwork, most compute-density operations are conducted in FP8, while a few key operations\\nare strategically maintained in their original data formats to balance training efficiency and\\nnumerical stability. The overall framework is illustrated in Figure 6.\\nFirstly, in order to accelerate model training, the majority of core computation kernels, i.e.,\\nGEMM operations, are implemented in FP8 precision. These GEMM operations accept FP8\\ntensors as inputs and produce outputs in BF16 or FP32. As depicted in Figure 6, all three GEMMs\\nassociated with theLinear operator, namelyFprop (forward pass), Dgrad (activation backward\\npass), and Wgrad (weight backward pass), are executed in FP8. This design theoretically doubles\\nthe computational speed compared with the original BF16 method. Additionally, the FP8 Wgrad\\nGEMM allows activations to be stored in FP8 for use in the backward pass. This significantly\\nreduces memory consumption.\\nDespite the efficiency advantage of the FP8 format, certain operators still require a higher\\nprecision due to their sensitivity to low-precision computations. Besides, some low-cost opera-\\ntors can also utilize a higher precision with a negligible overhead to the overall training cost. For\\nthis reason, after careful investigations, we maintain the original precision (e.g., BF16 or FP32)\\nfor the following components: the embedding module, the output head, MoE gating modules,\\nnormalization operators, and attention operators. These targeted retentions of high precision\\nensure stable training dynamics for DeepSeek-V3. To further guarantee numerical stability, we\\nstore the master weights, weight gradients, and optimizer states in higher precision. While\\n15\")\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Group Relative Policy Optimization (GRPO) is a technique used in training machine learning models, specifically in reinforcement learning. Let's break it down in simple terms:\n",
      "\n",
      "### What is Reinforcement Learning?\n",
      "Reinforcement learning is a type of machine learning where an agent learns to make decisions by performing actions in an environment to achieve a goal. The agent receives rewards or penalties based on its actions, and it uses this feedback to improve its future decisions.\n",
      "\n",
      "### What is Policy Optimization?\n",
      "Policy optimization is a method in reinforcement learning where the agent's strategy (policy) for making decisions is improved over time. The goal is to find the best policy that maximizes the total reward the agent can get.\n",
      "\n",
      "### What is GRPO?\n",
      "Group Relative Policy Optimization (GRPO) is a specific way to optimize the policy of an agent. Here's how it works:\n",
      "\n",
      "1. **Sampling Outputs**: For each question or task, GRPO samples a group of possible outputs (answers or actions) from the current policy model. This means it generates several different responses to the same question.\n",
      "\n",
      "2. **Evaluating Outputs**: Each of these outputs is evaluated to see how good they are. This evaluation is based on the rewards they would receive.\n",
      "\n",
      "3. **Calculating Advantage**: The advantage of each\n"
     ]
    }
   ],
   "source": [
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "while True:\n",
    "    input_message = input(\"Enter your query: \\n\")\n",
    "    if input_message == \"end\": break\n",
    "\n",
    "    for step in graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "        stream_mode=\"values\",\n",
    "        config=config,\n",
    "    ):\n",
    "        step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56ea0486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 00:35:21.612 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-28 00:35:21.620 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-28 00:35:21.630 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-28 00:35:21.631 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-28 00:35:21.631 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-28 00:35:21.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-28 00:35:21.663 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# Initialize chat history in session state\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "# Display existing chat messages\n",
    "for message in st.session_state.chat_history:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Chat input box\n",
    "if prompt := st.chat_input(\"Enter your query:\"):\n",
    "    # Append user message to chat history\n",
    "    st.session_state.chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Placeholder for assistant's response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        response_container = st.empty()\n",
    "\n",
    "        # Stream the response from LangChain\n",
    "        def generate_response():\n",
    "            for step in graph.stream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
    "                stream_mode=\"values\",\n",
    "                config=config,\n",
    "            ):\n",
    "                content = step[\"messages\"][-1].content\n",
    "                yield content\n",
    "\n",
    "        # Display the streaming response\n",
    "        full_response = st.write_stream(generate_response())\n",
    "        # Append assistant's response to chat history\n",
    "        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": full_response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c3d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
